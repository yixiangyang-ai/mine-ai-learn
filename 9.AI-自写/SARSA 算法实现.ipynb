{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "# sarsa learning\ndef learning(self, gamma, alpha, max_episode_num):\n        # self.Position_t_name, self.reward_t1 \u003d self.observe(env)\n        total_time, time_in_episode, num_episode \u003d 0, 0, 0\n        while num_episode \u003c max_episode_num: # 设置终止条件\n            self.state \u003d self.env.reset()    # 环境初始化\n            s0 \u003d self._get_state_name(self.state) # 获取个体对于观测的命名\n            self.env.render()                # 显示UI界面\n            a0 \u003d self.performPolicy(s0, num_episode, use_epsilon \u003d True)\n\n            time_in_episode \u003d 0\n            is_done \u003d False\n            while not is_done:               # 针对一个Episode内部\n                # a0 \u003d self.performPolicy(s0, num_episode)\n                s1, r1, is_done, info \u003d self.act(a0) # 执行行为\n                self.env.render()            # 更新UI界面\n                s1 \u003d self._get_state_name(s1)# 获取个体对于新状态的命名\n                self._assert_state_in_Q(s1, randomized \u003d True)\n                # 获得A\u0027\n                a1 \u003d self.performPolicy(s1, num_episode, use_epsilon\u003dTrue)\n                old_q \u003d self._get_Q(s0, a0)  \n                q_prime \u003d self._get_Q(s1, a1)\n                td_target \u003d r1 + gamma * q_prime  \n                #alpha \u003d alpha / num_episode\n                new_q \u003d old_q + alpha * (td_target - old_q)\n                self._set_Q(s0, a0, new_q)\n\n                if num_episode \u003d\u003d max_episode_num: # 终端显示最后Episode的信息\n                    print(\"t:{0:\u003e2}: s:{1}, a:{2:2}, s1:{3}\".\\\n                        format(time_in_episode, s0, a0, s1))\n\n                s0, a0 \u003d s1, a1\n                time_in_episode +\u003d 1\n\n            print(\"Episode {0} takes {1} steps.\".format(\n                num_episode, time_in_episode)) # 显示每一个Episode花费了多少步\n            total_time +\u003d time_in_episode\n            num_episode +\u003d 1\n        return\ndef main():\n    env \u003d SimpleGridWorld()\n    agent \u003d Agent(env)\n    print(\"Learning...\")  \n    agent.learning(gamma\u003d0.9, \n                   alpha\u003d0.1, \n                   max_episode_num\u003d800)\n\nif __name__ \u003d\u003d \"__main__\":\n    main()"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}