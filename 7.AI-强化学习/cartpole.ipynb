{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Episode finished after 14 timesteps\n",
            "Episode finished after 14 timesteps\n",
            "Episode finished after 21 timesteps\n",
            "Episode finished after 24 timesteps\n",
            "Episode finished after 15 timesteps\n",
            "Episode finished after 25 timesteps\n",
            "Episode finished after 13 timesteps\n",
            "Episode finished after 22 timesteps\n",
            "Episode finished after 37 timesteps\n",
            "Episode finished after 25 timesteps\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import gym\nimport numpy as np\nenv \u003d gym.make(\u0027CartPole-v0\u0027)\nfor i_episode in range(10):\n    observation \u003d env.reset()#重置环境\n    for t in range(100):\n        env.render()\n        #print(observation)\n        action \u003d env.action_space.sample()\n        observation, reward, done, info \u003d env.step(action)\n        if done:\n            print(\"Episode finished after {} timesteps\".format(t+1))\n            break\nenv.close()\n\ndef run_episode(env, policy, gamma \u003d 0.9, render \u003d False):\n    \"\"\" Runs an episode and return the total reward \"\"\"\n    obs \u003d env.reset()\n    total_reward \u003d 0\n    step_idx \u003d 0\n    while True:\n        if render:\n            env.render()\n        obs, reward, done , _ \u003d env.step(int(policy[obs]))\n        total_reward +\u003d (gamma ** step_idx * reward)\n        step_idx +\u003d 1\n        if done:\n            break\n    return total_reward\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "#在第一个小栗子中，使用了 env.step() 函数来对每一步进行仿真，在 Gym 中，env.step() 会返回 4 个参数：\n\n#观测 Observation (Object)：当前 step 执行后，环境的观测(类型为对象)。例如，从相机获取的像素点，机器人各个关节的角度或棋盘游戏当前的状态等；\n\n#奖励 Reward (Float): 执行上一步动作(action)后，智体(agent)获得的奖励(浮点类型)，不同的环境中奖励值变化范围也不相同，但是强化学习的目标就是使得总奖励值最大；\n\n#完成 Done (Boolen): 表示是否需要将环境重置 env.reset。大多数情况下，当 Done 为 True 时，就表明当前回合(episode)或者试验(tial)结束。例如当机器人摔倒或者掉出台面，就应当终止当前回合进行重置(reset);\n\n#信息 Info (Dict): 针对调试过程的诊断信息。在标准的智体仿真评估当中不会使用到这个 info，具体用到的时候再说。\n\n#总结来说，这就是一个强化学习的基本流程，在每个时间点上，智体执行 action，环境返回上一次 action 的观测和奖励，用图表示为"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Policy-Iteration converged at step 7.\nAverage scores \u003d  0.9\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "def evaluate_policy(env, policy, gamma \u003d 1.0, n \u003d 100):\n    scores \u003d [run_episode(env, policy, gamma, False) for  _ in range(n)]\n    return np.mean(scores)\ndef extract_policy(v, gamma \u003d 1.0):\n    \"\"\" Extract the policy given a value-function \"\"\"\n    policy \u003d np.zeros(env.nS)\n    for s in range(env.nS):\n        q_sa \u003d np.zeros(env.nA)\n        for a in range(env.nA):\n            q_sa[a] \u003d sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n        policy[s] \u003d np.argmax(q_sa)\n    return policy\ndef compute_policy_v(env, policy, gamma\u003d1.0):\n    \"\"\" Iteratively evaluate the value-function under policy.\n    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n    and solve them to find the value function.\n    \"\"\"\n    v \u003d np.zeros(env.nS)\n    eps \u003d 1e-10\n    while True:\n        prev_v \u003d np.copy(v)\n        for s in range(env.nS):\n            policy_a \u003d policy[s]\n            v[s] \u003d sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n        if (np.sum((np.fabs(prev_v - v))) \u003c\u003d eps):\n            # value converged\n            break\n    return v\n    return policy\ndef policy_iteration(env, gamma \u003d 1.0):\n    \"\"\" Policy-Iteration algorithm \"\"\"\n    policy \u003d np.random.choice(env.nA, size\u003d(env.nS))  # initialize a random policy\n    max_iterations \u003d 200000\n    gamma \u003d 1.0\n    for i in range(max_iterations):\n        old_policy_v \u003d compute_policy_v(env, policy, gamma)\n        new_policy \u003d extract_policy(old_policy_v, gamma)\n        if (np.all(policy \u003d\u003d new_policy)):\n            print (\u0027Policy-Iteration converged at step %d.\u0027 %(i+1))\n            break\n        policy \u003d new_policy\n    return policy\nif __name__ \u003d\u003d \u0027__main__\u0027:\n    env_name  \u003d \u0027FrozenLake8x8-v0\u0027\n    env \u003d gym.make(env_name)\n    optimal_policy \u003d policy_iteration(env, gamma \u003d 1.0)\n    scores \u003d evaluate_policy(env, optimal_policy, gamma \u003d 1.0)\n    print(\u0027Average scores \u003d \u0027, np.mean(scores))\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}